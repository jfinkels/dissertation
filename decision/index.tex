\newcommand{\PCPcs}[5]{\PCP^{#1}_{#2, #3}\left[#4, #5\right]}
\newcommand{\loglog}{\log\log}
\newcommand{\ceil}[1]{\left\lceil{#1}\right\rceil}
\newcommand{\FSAT}{\textsc{FSat}}

\chapter{Decision problems}
\label{chp:decision}

% % Foreword %
%
% %% Context (anyone - why now?) %%
%
% What is the current situation, and why is the need so important?
%
\lettrine[loversize=0.1, lhang=0.05, findent=0.2em, nindent=0em]{O}{ne of the major successes} of the PCP theorem, a characterization of $\NP$ as a class of computational problems that have probabilistically checkable proof systems (with polynomial-time verifiers), is that it provides a route to proving that approximating certain computationally intractable optimization problems is as difficult as solving them exactly.
%
% %% Need (readers - why you?) %%
%
% Why is this relevant to the reader, and why does something need to be done?
% (Also reference relevant existing work.)
%
The growth of multiprocessor systems in both general purpose personal computing and large-scale big data computations highlights the urgency of proving the analagous inapproximability (or approximability) of inherently sequential optimization problems by highly parallel algorithms.
However, there has been little theoretical work toward proving parallel inapproximability.
Unfortunately, the techniques used to prove the original PCP theorem rely on the fact that $\NP$ can be interpreted as the class of languages for which there is an efficient verification procedure given a brief witness to language membership; no such obvious interpretation of $\P$ exists.

If we consider the $\P$-complete problems to be tractable but inherently sequential and $\NC$ problems to be highly parallelizable, then our guiding question is whether there is a probabilistically checkable proof (PCP) characterization of $\P$ with $\NC$ verifiers.
Such a characterization would potentially provide a path to proving parallel inapproximability.
Indeed, this question was already on the minds of researchers such as Luca~Trevisan soon after the original proof of the PCP theorem.
\begin{quote}
  An intriguing question is whether the known non-approximability results for sequential algorithms can be improved when we restrict to $\NC$ algorithms (under the assumption that $\P \neq \NC$).
  A possible way may be to devise \emph{probabilistic proof systems for $\P$} more efficient that the currently known proof systems for $\NP$.
  Such a result would have a great independent interest.
  However, it is not clear why proofs for $\P$ should be \emph{easier to check} than proofs for $\NP$ (they only appear to be \emph{easier to generate}).~\autocite{trevisan98}
\end{quote}
%
% %% Task (author - why me?) %%
%
% What was undertaken to address the need?
%
As a first step towards characterizing probabilistic proof systems for $\P$,
%
% %% Object (document - why this document?) %%
%
% What does this document cover?
%
this chapter provides some initial structural complexity results for classes of probabilistically checkable proof systems for nondeterministic $\NC$ circuit families

%
% % Summary %
%
% %% Findings (author - what?)
%
% What did the work reveal when performing the task?
%
Perhaps $\P$ has proof systems that are easy to check in $\NC$, but this remains unclear.
Instead, we consider proof systems for the class $\NNC[\polylog]$, the class of languages decidable by $\NC$ circuit families augmented with a polylogarithmic number of nondeterministic gates.
%% We hope that this will be a valuable first step toward understanding proof systems for $\P$.
Other researchers such as Jonathan~Buss and Judy~Goldsmith have had similar questions about classes like this.
\begin{quote}
  The fundamental question remains whether there are problems in $\P$ that can be computed more quickly with limited nondeterminism than without it.~\autocite{bg93}
\end{quote}
We consider $\NNC[\polylog]$ for two reasons.
First, it is defined in such a way that it explicitly has short proof systems which are easy to verify in parallel, just as $\NP$ is defined in such a way that it explicitly has short proof systems which are easy to verify efficiently.
Second, it, like $\P$, lies between $\NC$ and $\NP$.

%
% %% Conclusion (readers - so what?)
%
% What did the findings mean for the audience?
%
Although our original intention was to show something like $\NNC[\polylog] = \PCP[O(\loglog n), O(1)]$, our research reveals that proving such an equality is equivalent to proving $\NNC[\polylog] = \NC$, or in other words, that a polylogarithmic amount of nondeterminism can be simulated deterministically by an $\NC$ circuit family.
This should be seen as evidence that such a result is unlikely; in fact, we show that such a simulation implies a deterministic subexponential time algorithm for the Boolean formula satisfiability problem!
We are still, however, able to show that certain PCP classes are contained in $\NNC[\polylog]$.

%
% %% Perspective (anyone - what now?)
%
% What should be done next?

%% Recent work by other authors has focused on the practicality of implementing proof systems for $\NP$ on real computers.
%% Unlike this work, in which we completely ignore the resources required for the prover to transform the proof of membership in a language to a probabilistically checkable proof in the appropriate format, other works focus on efficient implementation of both the verifier and the prover.
%% Also, some other works consider a model of proof system in which the prover and the verifier have some limited communication.
%% See \autocites{bcgt13}{gkr08}{smbw12}{svpbbw12}{trmp12} for more information.

\section{History}

In the 1990s, the PCP theorem by Arora, et al. \autocite{almss92}, the culmination of a line of research attempting to trade nondeterminism for randomness in nondeterministic polynomial-time algorithms, provided a surprising new technique for verifying a mathematical proof: as long as the proof is converted to a certain format, an algorithm using a small amount of randomness can decide whether the proof is correct by examining a constant number of bits of the proof.
On top of this fascinating illustration of the power of randomness in computation and mathematics, the PCP theorem provides a theoretical basis for proving inapproximability of $\NP$ optimization problems by polynomial-time algorithms.
In 1991, Feige, et al. \autocite{fglss91} used a generic gap-introducing reduction from a probabilistically checkable proof system to the maximum clique problem to show that the problem is hard to approximate by a polynomial-time algorithm.
From there, any approximation-preserving reduction (see \autoref{chp:optimization}) from the maximum clique problem to another optimization problem proves a similar level of inapproximability.

Other researchers examined different settings of parameters for the PCP theorem.
For example, in 1996, Fotakis and Spirakis \autocite{fs96} provided a lower bound on the amount of randomness needed when creating a PCP verifier for an $\NP$ problem.
%% ??? showed that the completeness and soundness probabilities of the verifier could be ???.
In 1998, Trevisan examined inapproximability by parallel algorithms instead of inapproximability by polynomial-time algorithms for the linear programming problem \autocite{trevisan98}.

The original proof of the PCP theorem is complicated and computational in nature.
Inspired by some techniques for constructing explicit constructions of expander graphs like that of Reingold, Vadhan, and Wigderson \autocite{rvw00}, in 2007, Dinur provided a simpler (and almost entirely combinatorial) proof of the PCP theorem \autocite{dinur07}.
%% \emph{TODO What is the status of a fully algebraic PCP theorem?}
More recent work on proof systems has focused on practical real-world implementations in which the prover and the verifier have some limited communication; see articles by
Goldwasser, Kalai, and Rothblum in 2008 \autocite{gkr08},
Setty et al. in 2012 \autocite{smbw12},
Setty et al. in 2012 \autocite{svpbbw12},
Thaler et al. in 2012 \autocite{trmp12},
and Ben-Sasson et al. in 2013 \autocite{bcgt13}.

Around the same time as the PCP theorem, Wolf studied the class of nondeterministic highly parallel algorithms, $\NNC$, and noted that a polylogarithmic amount of nondeterminism was an ``interesting'' amount of nondeterminism, suggesting that such a class may be incomparable with $\P$ \autocite{wolf94}.
The deterministic class $\NC$, an abbreviation of \emph{Nick's Class} in honor of Nick~Pippenger, has been considered the class of decision problems that admit highly parallel algorithms since the 1970s; for a more detailed history of the study of parallel versus sequential computation, see \autocite[Section~1.3]{ghr95}.
The complexity classes $\NNC^k[\log^i n]$ were proven to have complete problems by Cai and Chen in 1997 \autocite{cc97lim}.
The study of limited nondeterminism for polynomial-time algorithms was initiated in 1980 by Kintala and Fisher \autocite{kf80}, and advanced by several other researchers. Perhaps the most relevant to this dissertation are the articles by Díaz and Torán in 1990 \autocite{dt90}, Buss and Goldsmith in 1993 \autocite{bg93}, and Cai and Chen in 1997 \autocite{cc97lim}.

\section{Preliminaries}

Throughout this chapter, $\log n$ denotes the base 2 logarithm of $n$.
In the definitions below, $\mathbb{N}$ denotes the set of non-negative integers and $\mathbb{R}$ denotes the set of real numbers.

\begin{definition}
  For all functions $f, g \colon \mathbb{N} \to \mathbb{R}$, the function $f$ is in the class $O(g(n))$ if there exist real numbers $c$ and $N$ such that for all natural numbers $n$ we have $n > N$ implies $f(n) \leq c \cdot g(n)$.
  If $f(n) < c \cdot g(n)$ then $f(n)$ is in $o(g(n))$.
  If $f(n) \geq c \cdot g(n)$ then $f(n)$ is in $\Omega(g(n))$.
  If $f(n) > c \cdot g(n)$ then $f(n)$ is in $\omega(g(n))$.
\end{definition}

We assume the reader knows the basic definitions from complexity theory, including those of the complexity classes $\P$, $\NP$, $\DTIME$, and $\DSPACE$.
We define the class $\L^k$ by $\L^k = \DSPACE(\log^k n)$ for all nonnegative integers $k$ and the class $\polyL$ by $\polyL = \cup_{k \in \mathbb{N}} \DSPACE(\log^k n)$.
We denote the class $\L^1$ by simply $\L$.
We define the complexity class $\SUBEXP$, the class of languages decidable by deterministic ``subexponential'' time Turing machines, as
\begin{equation*}
  \SUBEXP = \bigcap_{\epsilon > 0} \DTIME(2^{n^\epsilon})
\end{equation*}
and \QP{}, the class of languages decidable by a deterministic ``quasipolynomial'' time Turing machine, as
\begin{equation*}
  \QP = \bigcup_{k \in \mathbb{N}} \DTIME(2^{\log^k n}).
\end{equation*}
We will also be considering $\NC^k$, the class of languages decidable by a family of logarithmic space uniform Boolean circuits of polynomial size, $O(\log^k n)$ depth, and unbounded fan-in.
We will denote by $\NC$ the union of all the $\NC^k$ classes.
A language in $\NC^k$ can also be described as a language which admits an algorithm which uses a polynomial number of processors running in $O(\log^k n)$ time on a parallel random-access machine (PRAM).
We describe $\NC$ algorithms using this paradigm.

\begin{definition}
  A \emph{probabilistically checkable proof verifier} (\emph{PCP verifier}) is a probabilistic Turing machine with sequential access to an input string $x$, sequential access to a random string $\rho$, and \emph{nonadaptive random access} to a proof string $\pi$.
\end{definition}

\begin{definition}
  Let $r(n)$ and $q(n)$ be bounded above by polynomials in $n$, and let $c(n)$ and $s(n)$ be functions whose values are in the interval $[0, 1]$.
  A language $L$ has a $(r(n), q(n), c(n), s(n))$-\emph{PCP verifier} if there exists a PCP verifier $V$ such that $V$ uses at most $r(n)$ bits of the random string $\rho$, makes at most $q(n)$ \emph{nonadaptive} queries to bits of the proof $\pi$, and satisfies the following conditions.
  \begin{enumerate}
  \item If $x \in L$, then
    \begin{equation*}
      \exists \pi \in \Sigma^* \colon \Pr_{\rho \in \Sigma^{r(n)}}{\left[V(x, \pi; \rho) \textnormal{ accepts}\right]} \geq c(n).
    \end{equation*}
  \item If $x \notin L$, then
    \begin{equation*}
      \forall \pi \in \Sigma^* \colon \Pr_{\rho \in \Sigma^{r(n)}}{\left[V(x, \pi; \rho) \textnormal{ accepts}\right]} < s(n).
    \end{equation*}
  \end{enumerate}
  The value $c(n)$ is the \emph{completeness} and the value $s(n)$ the \emph{soundness} of the verifier.
\end{definition}

In this chapter, we will consider only nonadaptive PCP verifiers.
Since a (nonadaptive) $(r(n), q(n), c(n), s(n))$-PCP verifier can read at most $2^{r(n)} q(n)$ locations of the proof string with nonzero probability, we assume without loss of generality that the proof provided to the verifier is of length at most $2^{r(n)} q(n)$ \autocite[Remark~11.6]{ab09}.
(Note that a verifier which uses $q(n)$ adaptive random access queries to the proof string can be simulated by a verifier which uses $2^{q(n)}$ nonadaptive random access queries to the proof string, so in the adaptive case, the proof string could be of length $2^{r(n) + q(n)}$.)

\begin{definition}
  Let $\PCPcs{\mathcal{C}}{c(n)}{s(n)}{r(n)}{q(n)}$ be the class of all languages $L$ such that $L$ has a $(r(n), q(n), c(n), s(n))$-PCP verifier $V$ computable by a $\mathcal{C}$ algorithm.

  More generally, if $\mathcal{F}$ and $\mathcal{G}$ are classes of functions,
  \begin{equation*}
    \PCPcs{\mathcal{C}}{c(n)}{s(n)}{\mathcal{F}}{\mathcal{G}} = \bigcup_{f \in \mathcal{F}, g \in \mathcal{G}}{\PCPcs{\mathcal{C}}{c(n)}{s(n)}{f(n)}{g(n)}},
    \end{equation*}

  Since completeness 1 and soundness \sfrac{\textpl{1}\kern-0.25em}{\kern-0.3em\textpl{2}} are common parameters, and for the sake of brevity, we write $\PCP^{\mathcal{C}}[r(n), q(n)]$ to denote $\PCPcs{\mathcal{C}}{1}{\frac{1}{2}}{r(n)}{q(n)}$, and $\PCP^{\mathcal{C}}[\mathcal{F}, \mathcal{G}]$ to denote $\PCPcs{\mathcal{C}}{1}{\frac{1}{2}}{\mathcal{F}}{\mathcal{G}}$.
\end{definition}

Please notice that the complexity class given in the superscript in the above definition does \emph{not} denote an oracle; it merely describes the computational power of the PCP verifier.

From the definition, we see immediately that
\begin{equation*}
  \PCPcs{\mathcal{C}}{c(n)}{s(n)}{O(r(n))}{O(q(n))} = \bigcup_{a \in \mathbb{N}, b \in \mathbb{N}}{\PCPcs{\mathcal{C}}{c(n)}{s(n)}{a \cdot r(n)}{b \cdot q(n)}}.
\end{equation*}

\section{Probabilistically checkable proofs for nondeterministic circuits}

We first provide a PCP characterization of $\NNC[\polylog]$, then later we provide upper and lower bounds for the randomness and query complexity parameters of such a PCP verifier.
The following theorem shows that a nondeterministic $\NC$ circuit can simulate a PCP verifier and vice versa with the appropriate tradeoff in parameters.

\begin{theorem}\label{thm:qplusr}
  For all nonnegative integers $q$ and $r$,
  $$
    \NNC[\log^q n] \subseteq \PCP^{\NC}[r \loglog n, O(\log^q n)] \subseteq \NNC[\log^{q + r} n].
  $$
\end{theorem}
\begin{proof}
  Let $q$ and $r$ be non-negative integers.
  The first inclusion, $\NNC[\log^q n] \subseteq \PCP^\NC[r \loglog n, O(\log^q n)]$, follows immediately from the definitions.
  For the other direction, suppose $L \in \PCP^{\NC}[r \loglog n, c \log^q n]$ for some constant $c$.
  Construct an $\NNC$ machine $M$ which proceeds as follows on input $x$ of length $n$.
  \begin{enumerate}
  \item Guess a proof string $\pi$ of length $2^{r \loglog n} c \log^q n$.
  \item For each $\rho$ of length $r \loglog n$ \emph{in parallel} simulate $V(x, \pi; \rho)$.
  \item Accept if and only if at least half of the simulations accept.
  \end{enumerate}
In the initial step, guessing a proof string requires $O(\log^{q + r} n)$ bits.
In the second step, since $V$ is an $\NC$ machine, a polylogarithmic number of parallel simulations of $V$ can be executed with only a polylogarithmic factor increase in size and no increase in depth.
In the final step, computing the majority of a polylogarithmic number of bits can be done by an $\NC$ circuit.
Therefore $M$ is an $\NNC[\log^{q + r} n]$ machine.
The correctness of $M$ follows from the completeness and soundness of $V$.
\end{proof}

Choosing $r = 1$ yields
$$
\NNC[\log^q n] \subseteq \PCP^{\NC}[\loglog n, O(\log^q n)] \subseteq \NNC[\log^{q + 1} n].
$$
On the other hand, allowing $r$ and $q$ to vary over the set of natural numbers proves the equality of the two hierarchies.

\begin{corollary}\label{cor:polylogeq}
  $\NNC[\polylog] = \PCP^{\NC}[O(\loglog n), \polylog]$.
\end{corollary}

Next, consider the chain of inclusions
\begin{equation*}
  \NNC[\log n] \subseteq \NNC[\polylog] \subseteq \NNC[\poly].
\end{equation*}
In fact, $\NC = \NNC[\log n]$ and $\NNC[\poly] = \NP$ \autocite{wolf94}, so we can rewrite this as
\begin{equation}\label{eq:chain}
  \NC \subseteq \NNC[\polylog] \subseteq \NP.
\end{equation}
We now wish to provide $\NC$ PCP characterizations for both $\NC$ and $\NP$.

In \autocite{fs96}, the authors prove that $\P = \PCP^\P[O(\loglog n), O(1)]$ (implicitly; they state only that $\NP = \PCP^\P[O(\loglog n), O(1)]$ if and only if $\P = \NP$).
The same proof techniques can be used in the $\NC$ setting with essentially no changes.
(The idea of the proof is to simulate $O(\loglog n)$ bits of randomness with $\loglog n + O(1)$ bits by making a random walk of an appropriate length on a fully explicit constant degree expander graph.)
This yields the following PCP characterization of $\NC$.

\begin{theorem}\label{thm:ncpcp}
  $\NC = \PCP^\NC[O(\loglog n), O(1)]$.
\end{theorem}

As a generalization of the result of \autocite{fs96}, we know $\NP = \PCP^\P[o(\log n), o(\log n)]$ if and only if $\P = \NP$ \autocites{as98}{fglss91}.
%The equality $\P = \PCP^\P[O(\loglog n), O(1)]$ is actually a special case of $\P = \PCP^\P[o(\log n), o(\log n)]$, which is proven (again implicitly) in \autocite{as98} using a reduction from \autocite{fglss91}.
Unfortunately, the obvious strategy for translating that proof to the $\NC$ setting fails.
The proof would have shown that $\NP = \PCP^{\NC^k}[o(\frac{\loglog n}{\log^k n}), O(1)]$ if and only if $\NC = \NP$, but this is already proven by \autoref{thm:ncpcp} and the fact that $\frac{\loglog n}{\log^k n} \leq \loglog n$.

We also know the following strengthening of the original PCP theorem; the proof of this theorem is in \autoref{sec:dinur}.
\begin{restatable}{theorem}{pcpnp}\label{thm:pcpnp}
  $\PCP^\NC[O(\log n), O(1)] = \NP$.
\end{restatable}

From \autoref{eq:chain}, \autoref{thm:ncpcp}, and \autoref{thm:pcpnp}, we have the two equivalent inclusion chains
$
  \NC \subseteq \NNC[\polylog] \subseteq \NP
$
and
\begin{equation*}
  \PCP[O(\loglog n), O(1)] \subseteq \PCP[O(\loglog n), \polylog] \subseteq \PCP[O(\log n), O(1)],
\end{equation*}
where the PCP verifier is an $\NC$ machine.
If we can provide evidence that $\NC \neq \NNC[\polylog]$ and that $\NNC[\polylog] \neq \NP$, we can conclude that the corresponding PCP classes are also likely distinct.
This theorem, adapted from \autocite[Theorem~1]{dt90} (therein attributed to R.~Beigel), provides that evidence.
%% TODO link to explanation of ETH, link to NP in QP implies EXP = NEXP
Each of the two conclusions in this theorem implies that the exponential time hypothesis is false.
Furthermore, in the latter case, the conclusion implies that $\EXP = \NEXP$.

\begin{theorem}\label{thm:npinqp}
  \mbox{}
  \begin{enumerate}
  \item If $\NC = \NNC[\polylog]$, then $\NP \subseteq \SUBEXP$.
  \item If $\NNC[\polylog] = \NP$, then $\NP \subseteq \QP$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  If $\NNC[\polylog] = \NP$, then
  \begin{align*}
    \NP & = \NNC[\polylog] \\
        & \subseteq \DSPACE(\polylog) && \text{by \autocite{wolf94}} \\
        & \subseteq \DTIME(2^{\polylog}) && \text{by exhaustive search} \\
        & = \QP && \text{by definition}.
  \end{align*}
  Now suppose $\NC = \NNC[\polylog]$.
  Since $\FSAT$, the Boolean formula satisfiability problem, is complete for $\NP$ under deterministic polynomial-time many-one reductions, it suffices to show a deterministic subexponential time algorithm for $\FSAT$.

  The proof uses a padding argument.
  First we observe that there is an $\NNC^1(n)$ machine, call it $M$, that decides $\FSAT$: given a Boolean formula $\phi$, guess a satisfying assignment to $\phi$ (of length $O(n)$) and evaluate the formula (Boolean formula evaluation is in $\NC^1$ \autocite{buss87}).
  Let $\epsilon$ be an arbitrarily small positive constant, and define $L$, the padded version of $\FSAT$, as
  \begin{equation*}
    L = \left\{ \phi \# 1^P \, \middle| \, \phi \in \FSAT \text{ and } P = 2^{n^\epsilon} - (n + 1) \right\},
  \end{equation*}
  where $n = |\phi|$.
  We claim $L$ is in $\NNC[\log^\frac{1}{\epsilon} n]$ by the following machine, $M_j$.
  On input $\phi'$, check that $\phi'$ is in the format $\phi \# 1^P$, then accept if and only if $M$ accepts $\phi$.
  The correctness of this algorithm follows from the correctness of $M$, so it remains to check the size and depth of the circuit for $M_j$, and the amount of nondeterminism used.

  Checking that $x'$ is in the correct format can be performed (deterministically) by an $\NC^1$ circuit by computing the conjunction of all the bits after the $\#$ symbol.
  Observe now that $|x'| = 2^{n^\epsilon}$, so $n = \log^\frac{1}{\epsilon}{|x'|}$.
  The amount of nondeterminism used by $M_j$ is the same as the amount used by $M$, which is $O(n)$, or $O(\log^\frac{1}{\epsilon} |x'|)$.
  The size of $M$ is polynomial in $n$, which is polylogarithmic in $|x'|$, and hence polynomial in the length of the input $x'$.
  The depth of $M$ is $O(\log n)$, which is $O(\log \log^\frac{1}{\epsilon} |x'|)$, or simply $O(\log \log |x'|)$.
  We conclude that the size of $M_j$ is polynomial in $|x'|$, the depth of $M_j$ is logarithmic in $|x'|$, and $M_j$ uses $O(\log^\frac{1}{\epsilon} |x'|)$ bits of nondeterminism.
  Hence $L$ is in $\NNC[\log^\frac{1}{\epsilon} n]$.

  By hypothesis, $L$ is also in $\NC$.
  Let $M_i$ be the $\NC$ machine that decides it.
  We claim that we can now construct a subexponential time algorithm for $\FSAT$ on inputs $\phi$ of length $n$.
  \begin{enumerate}
  \item Let $\phi' = \phi \# 1^P$, where $P = 2^{n^\epsilon} - (n + 1)$.
  \item Accept if and only if $M_i$ accepts $\phi'$.
  \end{enumerate}
  The correctness of this algorithm follows immediately from the correctness of $M_i$.
  The first step can be performed by a deterministic algorithm running in time $2^{n^\epsilon}$.
  The second step can be performed by an $\NC$ machine.
  Since $\NC \subseteq \P$, and $2^{n^\epsilon}$ is greater than any polynomial for sufficiently large $n$, the first step is the bottleneck in this algorithm.
  Therefore, this algorithm for $\FSAT$ can be implemented by a deterministic algorithm running in $O(2^{n^\epsilon})$ time for arbitrarily small $\epsilon$.
\end{proof}

As mentioned above, $\NC = \NNC[\log n]$ and $\NP = \NNC[\poly]$, so the two items in this theorem can be restated as follows.
\begin{enumerate}
\item If $\NNC[\log n] = \NNC[\polylog]$, then $\NP \subseteq \SUBEXP$.
\item If $\NNC[\polylog] = \NNC[\poly]$, then $\NP \subseteq \QP$.
\end{enumerate}
The first item indicates that a simulation of a polylogarithmic number of bits of nondeterminism by only a logarithmic number of bits is unlikely.
The second item indicates that a simulation of a polynomial number of bits of nondeterminism by only a polylogarithmic number of bits is unlikely.
The consequences of the latter simulation are more extreme (a simulation of $\NP$ in quasipolynomial time as opposed to a simulation of $\NP$ in subexponential time).

Substituting the PCP characterizations of each nondeterministic $\NC$ complexity class in the previous theorem provides evidence against the simulation of certain resources in probabilistically checkable proof systems.

\begin{corollary}
  \mbox{}
  \begin{enumerate}
  \item If $\PCP^\NC[O(\loglog n), O(1)] = \PCP^\NC[O(\loglog n), \polylog]$, then \\ $\NP \subseteq \SUBEXP$.
  \item If $\PCP^\NC[O(\loglog n), \polylog] = \PCP^\NC[O(\log n), O(1)]$, then $\NP \subseteq \QP$.
  \end{enumerate}
\end{corollary}

The first part of this corollary provides evidence that for certain classes of computational problems, an $\NC$ PCP verifier cannot reduce the number of necessary queries.
(However, it could still be the case that for some fixed positive integer $k$, we have $\PCP^\NC[O(\loglog n), O(1)] = \PCP^\NC[O(\loglog n), O(\log^k n)]$; see \autoref{con:smallqueries} below.)
The second part provides evidence that for certain classes of computational problems, a verifier cannot reduce randomness in exchange for an increase in the number of necessary queries.
Contrast this with \autocite[Corollary~10]{fs96} which states that
\begin{equation*}
  \PCP^\NC[O(\log^k \log n), O(\log^d \log n)] \subseteq \PCP^\NC[O(\loglog n), O(\log^{d + k - 1} \log n)]
\end{equation*}
(the result is proven for polynomial-time verifiers, but it holds for $\NC$ verifiers as well).
This yields the equality
\begin{equation*}
  \PCP^\NC[\poly(\loglog n), \poly(\loglog n)] = \PCP^\NC[O(\loglog n), \poly(\loglog n)],
\end{equation*}
which provides an even more severe collapse, assuming the following conjecture.
\begin{conjecture}\label{con:smallqueries}
  $\PCP^\NC[O(\loglog n), O(\log n)] = \PCP^\NC[O(\loglog n), O(1)]$.
\end{conjecture}
This is a scaled down version (that is, scaled from a $\P$ verifier down to an $\NC$ verifier) of some of the results of the research which led to the original PCP theorem.
If this conjecture holds, then $\poly(\loglog n)$ randomness and a logarithmic number of queries to the proof can be simulated deterministically.
\begin{theorem}
  If \autoref{con:smallqueries} holds, then $\PCP^\NC[\poly(\loglog n), O(\log n)] = \NC$.
\end{theorem}
\begin{proof}
  Combining \autoref{con:smallqueries} with the fact that $O(\log^\alpha \log n) \subseteq O(\log n)$ for all nonnegative integers $\alpha$, we have
  \begin{align*}
    \NC & \subseteq \PCP^\NC[\poly(\loglog n), \poly(\loglog n)] \\
    & \subseteq \PCP^\NC[O(\loglog n), \poly(\loglog n)] \\
    & \subseteq \PCP^\NC[O(\loglog n), O(\log n)] \\
    & \subseteq \PCP^\NC[O(\loglog n), O(1)] \\
    & \subseteq \NC.
    \qedhere
  \end{align*}
\end{proof}

Now we return to our original goal, finding a PCP characterization of $\P$.
The classes $\P$ and $\NNC[\polylog]$ are conjectured incomparable \autocite{wolf94}.
Using the results above, this conjecture implies that $\P$ and $\PCP^\NC[O(\loglog n), \polylog]$ are incomparable.
\autoref{thm:pinpcp} shows the negative consequences of a PCP characterization for $\P$.

\begin{theorem}\label{thm:pinpcp}
  \mbox{}
  \begin{enumerate}
  \item If $\P \subseteq \PCP^\NC[O(\loglog n), \polylog]$ then $\P \subsetneq \polyL$.
  \item If $\polyL \subsetneq \P$ then $\PCP^\NC[O(\loglog n), \polylog] \subsetneq \P$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  These implications are a consequence of three facts.
  \begin{enumerate}
  \item $\PCP^\NC[O(\loglog n), \polylog] \subseteq \NNC[\polylog]$ (\autoref{cor:polylogeq}).
  \item $\NNC[\polylog] \subseteq \polyL$ (\autocite[Corollary~3.2]{wolf94}).
  \item $\P \neq \polyL$ (\autocite[Theorem~3.10]{book76}). \qedhere
  \end{enumerate}
\end{proof}

Although $\P \neq \polyL$, whether one is a strict subset of the other remains unknown; the two are conjectured to be incomparable \autocite[Section~2.5.1]{johnson90}.
If $\P \subsetneq \polyL$, then $\P \subsetneq \QP$ (by exhaustive search over the quasipolynomial number of configurations of the $\polyL$ machine).
If $\polyL \subsetneq \P$, then $\L \subsetneq \L^2 \subsetneq \dotsb \subsetneq \polyL \subsetneq \P$.

\section{Inapproximability from PCPs}

Consider the \emph{maximum high degree subgraph problem} (\autocite{am84}): given an undirected graph $G$ find the largest integer $d$ such that $G$ has a vertex-induced subgraph of minimum degree $d$.
This is a relaxation of the maximum clique problem, in which the minimum degree of the induced subgraph $S$ is required to be at least $|S| - 1$.
There is a simple polynomial-time algorithm that outputs optimal solutions for this problem: repeatedly remove vertices of degree less than $d$ from the graph.
The subgraph that remains has minimum degree $d$; for more information, see \autocite[Problem~A.2.7]{ghr95}.

We know that the clique problem is inapproximable via a gap-introducing reduction from an arbitrary probabilistically checkable proof system \autocite{fglss91}.
It would be satisfying to use a similar reduction to provide a gap-introducing reduction from our restricted PCPs to the maximum high degree subgraph problem.
However, research in this direction failed to reveal such a reduction.

%% \section{Approximability implies simulation of PCP}

%% \begin{definition}
%%   Suppose $P$ is an optimization problem with $P = (I, S, m, t)$ and $\mathcal{C}$ is a complexity class.
%%   $P$ is \emph{$\mathcal{C}$-additive} if there is a binary operation $\oplus$ on $I$ such that for all $x_1$ and $x_2$ in $I$,
%%   \begin{enumerate}
%%     %% TODO should this first condition allow O()?
%%   \item $m^*(x_1 \oplus x_2) = m^*(x_1) + m^*(x_2)$,
%%   \item $|x_1 \oplus x_2| = O(|x_1| + |x_2|)$, and
%%   \item $\oplus$ is computable by a $\mathcal{C}$ algorithm.
%%   \end{enumerate}
%% \end{definition}

%% \begin{definition}
%%   Suppose $P$ is an optimization problem with $P = (I, S, m, t)$.
%%   For any instance $x$ in $I$, the \emph{parameterized solution set of $x$}, denoted $S_{\geq k}(x)$, is defined by $S_{\geq k}(x) = \{ y \in S(x) \,|\, m(x, y) \geq k \}$.
%%   The \emph{parameterized existence problem} for $P$, denoted $P_{\exists \geq k}$, is defined by $P_{\exists \geq k} = \{ x \in I \,|\, S_{\geq k}(x) \neq \emptyset \}$.
%% \end{definition}

%% \begin{definition}
%%   Suppose $\mathcal{C}$ is a complexity class and $c$ and $s$ are real numbers in $[0, 1]$.
%%   The \emph{acceptance problem for $\PCP^\mathcal{C}_{c, s}$ verifiers} is defined as
%%   \begin{equation*}
%%     \left\{(V, x, \rho) \, \middle| \, V \text{ is a $\PCP^\mathcal{C}_{c, s}$ verifier and } \, \exists \pi \colon V(x, \pi; \rho) \text{ accepts}\right\}.
%%   \end{equation*}
%% \end{definition}

%% The following theorem is a generalization of \autocite[Theorem~18]{trevisan98} that uses approximability of problems for which an exact solution is infeasible to construct an efficient deterministic simulation of PCP verifiers.

%% \begin{todo}
%%   There is a problem with the following theorem.
%%   The way we have defined the acceptance problem has the existential quantification for $\pi$ after the random string $\rho$ has been chosen.
%%   This means that the we could have a different proof string $\pi$ for each random string $\rho$.
%%   This does not correspond to the PCP model, in which a single $\pi$ exists that works for many random strings $\rho$.
%%   We need to somehow ensure that the reduction ensures that all the reduced instances have parts that are consistent with a single proof string.
%% \end{todo}

%% \begin{theorem}
%%   Suppose $P$ is an optimization problem in \NPO, and suppose $c$ and $s$ are real numbers with $0 \leq s \leq 0.5 < c \leq 1$.
%%   If
%%   \begin{enumerate}
%%   \item $P$ is \P-additive,
%%     %% TODO can we allow just the budget problem here?
%%   \item for some positive integer $k$, there is a polynomial time many-one reduction from the acceptance problem for $\PCP^\P_{c, s}$ verifiers to $P_{\exists \geq k}$, and
%%   \item $P$ has a polynomial time $\frac{c}{s}$-approximator,
%%   \end{enumerate}
%%   then $\PCP^\P_{c, s}[r \log n, q] \subseteq \P$.
%% \end{theorem}
%% \begin{proof}
%%   Suppose $L \in \PCP^\P_{c, s}[r \log n, q]$, and let $V$ be the PCP verifier that decides $L$.
%%   Let $\oplus$ be the operation guaranteed by the first condition of the hypothesis, $T$ the reduction guaranteed by the second, and $A$ the approximator guaranteed by the third.
%%   Let $m$ be the measure function of the optimization problem $P$.

%%   Construct deterministic Turing machine $M$ that proceeds as follows on inputs $x$ of length $n$.
%%   \begin{enumerate}
%%   \item For each string $\rho$ of length $r \log n$, let $z_i = T(V, x, \rho)$.
%%   \item Let $Z = \bigoplus_{i = 1}^{n^r} z_i$.
%%   \item Accept if and only if $m(Z, A(Z)) \geq k s n^r$.
%%   \end{enumerate}

%%   This algorithm halts in polynomial time (note that $|Z|$ is in $O(n^r \max_i |z_i|)$, polynomial in $n$ since the size of each $z_i$ is).
%%   If $x \in L$ then at least $c n^r$ of the $(V, x, \rho)$ correspond to accepting computations, so the same number of the $z_i$ have $m^*(z_i) \geq k$.
%%   By the additivity of $P$ we have
%%   \begin{equation*}
%%     m^*(Z) = \sum_{i = 1}^{n^r} m^*(z_i) \geq k c n^r.
%%   \end{equation*}
%%   By the $\frac{c}{s}$-approximability of $P$, we have
%%   \begin{equation*}
%%     \frac{c}{s} \geq \frac{m^*(Z)}{m(Z, A(Z))} \geq \frac{k c n^r}{m(Z, A(Z))}.
%%   \end{equation*}
%%   By rearranging this inequality, we find $m(Z, A(Z)) \geq k s n^r$, and hence the machine $M$ accepts $x$.
%%   On the other hand, if $x \notin L$ then by similar reasoning, $m(Z, A(Z)) \leq m^*(Z) < k s n^r$, and thus $M$ rejects $x$.
%%   We conclude that $M$ is a correct deterministic polynomial time Turing machine that decides $L$, and therefore $\PCP^\P_{c, s}[r \log n, q] \subseteq \P$
%% \end{proof}

%% %% We can translate this result first to the setting of somewhat parallel approximations for infeasible problems, then to highly parallel approximations for somewhat parallel problems, and finally to highly parallel approximations for inherently sequential problems.
%% A similar proof applies for $\NC$ simulation of restricted PCP verifiers.

%% \begin{corollary}
%%   Suppose $P$ is an optimization problem in $\NNCO(\polylog)$, and suppose $c$ and $s$ are real numbers with $0 \leq s \leq 0.5 < c \leq 1$.
%%   If
%%   \begin{enumerate}
%%   \item $P$ is $\NC$-additive,
%%     %% TODO can we allow just the budget problem here?
%%   \item for some positive integer $k$, there is an $\NC$ many-one reduction from the acceptance problem for $\PCP^\NC_{c, s}$ verifiers to $P_{\exists \geq k}$, and
%%   \item $P$ has an $\NC$ $\frac{c}{s}$-approximator,
%%   \end{enumerate}
%%   then $\PCP^\NC_{c, s}[r \log n, q] \subseteq \NC$.
%% \end{corollary}

\section{Probabilistically checkable proofs for nondeterministic polynomial time}
\label{sec:dinur}

One method of showing $\PCP^\NC[O(\log n), O(1)] = \NP$ is to revisit a proof of the PCP theorem and ensure that all computation can be performed by an $\NC$ PCP verifier without affecting the correctness of the proof.
We will consider Dinur's proof of the PCP theorem \autocite{dinur07}, which reduces the problem of proving $\PCP^\P[O(\log n), O(1)] = \NP$ to the problem of showing \textsc{$\frac{1}{2}$-gap $q$-CSP} is hard for $\NP$.
Meir observes that although we would like a verifier running in polylogarithmic time, Dinur's proof requires $O(\log n)$ iterations of a polynomial-time procedure, which yields a polynomial-time procedure \autocite[Section~1.2.1]{meir09}.
We show that a closer look reveals that parallel polylogarithmic time is indeed possible without any new machinery.

The proof provides a gap-introducing reduction from an arbitrary $\NP$ problem to a constraint satisfaction problem.
Let us first define the notion of a combinatorial constraint and when a constraint is satisfied.

\begin{definition}[{\autocite[Definition~1.1]{dinur07}}]
  Let $V$ be a finite set of variables, defined by $V = \{x_1, x_2, \dotsc, x_n\}$, let $\Gamma$ be a finite alphabet, and let $q$ be a natural number.
  A \emph{$q$-ary constraint} is a $q + 1$ tuple, $(C, i_1, i_2, \dotsc, i_q)$, where $C \subseteq \Gamma^q$ and each $i_k$ is the index of a variable in $V$.
  Here, $C$ is considered the set of ``acceptable'' values for the variables and each $i_k$ is the index of a variable whose assigned value will be checked against $C$.

  An \emph{assignment} is a function $a \colon V \to \Gamma$.
  An assignment \emph{satisfies} a constraint if $(a(v_{i_1}), a(v_{i_2}), \dotsc, a(v_{i_q})) \in C$.
\end{definition}

A natural question for a given set of constraints is whether there is an assignment to the variables that simultaneously satisfies all the constraints.
A related problem asks the same question but given the guarantee that either all the constraints are satisfied or few of the constraints are satisfied, regardless of the assignment.

\begin{definition}[\textsc{$q$-CSP}]
  \mbox{} \\
  \begin{tabular}{r p{9.5cm}}
    \textbf{Instance:} & finite alphabet $\Gamma$ with $|\Gamma| > 1$, finite set of variables $V$, finite set of $q$-ary constraints $D$. \\
    \textbf{Question:} & Are all constraints in $D$ satisfiable?
  \end{tabular}
\end{definition}

\begin{definition}[\textsc{$\frac{1}{2}$-gap $q$-CSP}]
  \mbox{} \\
  \begin{tabular}{r p{9.5cm}}
    \textbf{Instance:} & finite alphabet $\Gamma$ with $|\Gamma| > 1$, finite set of variables $V$, finite set of $q$-ary constraints $D$ with the restriction that for any assignment, either all constraints are satisfied or fewer than half are. \\
    \textbf{Question:} & Are all constraints in $D$ satisfiable?
  \end{tabular}
\end{definition}

In the special case in which $q = 2$, that is, all constraints are binary, we may interpret an instance of the constraint satisfaction problem as an undirected graph with vertex set $V$ and an edge labeled $C$ between vertices $v_i$ and $v_j$ for each $(C, i, j)$ in $D$.
We call such a graph a \emph{constraint graph} and we consider the size of this graph to be $|V| + |E|$ where $V$ is the set of vertices (equivalently, variables) and $E$ is the set of edges.

\begin{lemma}\label{lem:inapprox}
  If there is a positive integer $q$ such that \textsc{$\frac{1}{2}$-gap $q$-CSP} is hard for $\NP$ under $\NC$ many-one reductions, then $\PCP^\NC[O(\log n), O(1)] = \NP$.
\end{lemma}
\begin{proof}
  One inclusion in the conclusion of the theorem is true unconditionally, following from the PCP theorem \autocite{almss92}.
  For the other inclusion, let $L$ be a language in $\NP$.
  By hypothesis there is a many-one reduction computable in $\NC$ from $L$ to \textsc{$\frac{1}{2}$-gap $q$-CSP}.
  We construct the PCP verifier as follows.
  \begin{enumerate}
  \item Compute the reduction to produce a set of constraints.
  \item Use $O(\log n)$ random bits to choose a constraint uniformly at random.
  \item Check that the constraint is satisfied by querying the proof string at the appropriate locations (the locations corresponding to the $q$ variables in the constraint).
  \end{enumerate}

  The first step is computable in $\NC$ by hypothesis.
  The second step uses $O(\log n)$ bits of randomness and a constant number of parallel steps.
  In the third step, $q$ processors, working in parallel, each read the index of a variable given in the chosen constraint, then retrieve the value of that variable given in the proof string; this takes at most $O(\log n)$ parallel time.
  Verifying that the constraint is satisfied by these values is just the problem of checking $q$ set membership queries in parallel, which again can be done in $O(\log n)$ time (a loose upper bound).
  The overall number of processors in this algorithm is polynomial and the overall parallel time is polylogarithmic, thus this an $\NC$ algorithm.

  It remains to show that the algorithm is correct.
  If $x \in L$ then all constraints are satisfiable, so there exists an assignment such that the verifier will accept on all random choices of the constraint.
  If $x \notin L$ then fewer than half of the constraints are satisfiable, so for any assignment the probability that the verifier will select a satisfied constraint is less than half.
  Therefore we have shown a correct PCP verifier with the appropriate parameters for an arbitrary language in $\NP$.
\end{proof}

Now we examine Dinur's proof that \textsc{$\frac{1}{2}$-gap $q$-CSP} is hard for $\NP$ \autocite{dinur07}.
That proof shows that the problem is hard under polynomial-time many-one reductions, but we show here that it is in fact hard under $\NC$ many-one reductions.
First, we claim without proof that \textsc{$q$-CSP} is hard for $\NP$ under $\NC$ many-one reductions (because the standard polynomial-time many-one reductions showing that it is \NP-complete are in fact computable in logarithmic space).
Next, consider (the high-level description of) the polynomial-time many-one reduction from \textsc{$q$-CSP} to \textsc{$\frac{1}{2}$-gap $q$-CSP}: given constraint graph $G_0$ as input, compute and output $G_{O(\log n)}$, where $G_{i + 1} = \mathcal{P}({\left(X(G_i)\right)}^t)$ and $t \in O(1)$.
Here, $X$ is a preprocessing function, the exponent $t$ denotes a constant number of constraint graph powering operations, and $\mathcal{P}$ denotes an assignment testing composition function.
If each of these three functions is computable by an $\NC$ algorithm, then $G_{i + 1}$ can be computed from $G_i$ by an $\NC$ algorithm, and hence so can $G_{O(\log n)}$ from $G_0$.
We will consider each of the three functions below.

The preprocessing function $X$ requires a \emph{mildly explicit} construction of a constant degree expander.
The standard definition of mildly explicit is that a representation of the graph (for example, its adjacency matrix) is computable in time polynomial in the number of vertices in the graph; for comparison, in a \emph{fully explicit} expander, the $i$th neighbor of vertex $v$ can be computed in time polynomial in the size of the binary representation of $v$, that is, polynomial in $\log n$ where $n$ is the number of nodes in the graph.
We will refer to graphs which meet these definitions as \emph{polynomial-time mildly explicit} and \emph{polynomial-time fully explicit}.
Since we are constructing an $\NC$ algorithm, we will require the representation of the graph to be computable by an $\NC$ algorithm.
More formally, we require an \emph{$\NC$ mildly explicit} graph, that is, a graph for which a representation can be computed by an $\NC$ algorithm with respect to input $n$, the number of nodes of the graph.
%Fortunately, an $\NC$ mildly explicit expander is equivalent to a polynomial-time fully explicit expander, a new equivalence which may be of independent interest when constructing constant-degree expanders in parallel.
Fortunately, a polynomial-time fully explicit expander implies an $\NC$ mildly explicit expander, a new implication that may be of independent interest when constructing constant-degree expanders in parallel.

\begin{proposition}
  Suppose $G$ is a $d$-regular expander graph.
  If $G$ is polynomial-time fully explicit, then it is $\NC$ mildly explicit.
\end{proposition}
\begin{proof}
  %% First suppose $G$ is $\NC$ mildly explicit, so there exists an $\NC$ algorithm that outputs, say, the adjacency list of the graph given $n$ as input.
  %% The following algorithm computes the $i$th neighbor of vertex $v$ in time polynomial in $\log n$: construct the adjacency list of the graph, then find and output the $i$th neighbor of vertex $v$.
  %% When constructing the adjacency list, we simulate each of the polynomial number of processors sequentially, each one running for $\poly(\log n)$ time, so the overall running time of this step remains $\poly(\log n)$.
  %% The list corresponding to vertex $v$ can be found in $O(\log n)$ steps (by using a binary search tree), and the $i$th element of that list can be found in constant time (since $i$ is bounded above by $d$, a constant).
  %% Therefore we have presented an algorithm which runs in $\poly(\log n)$ time which correctly computes the $i$th number of $v$ in the graph $G$.

  Suppose $G$ is polynomial-time fully explicit, so there exists an algorithm that computes the $i$th neighbor of $v$ in time polynomial in $\log n$.
  Let $f(v, i)$ denote this algorithm.
  The following $\NC$ algorithm computes the adjacency list of $G$ given the number of nodes $n$: for each vertex $v$ \emph{in parallel} and each index $i$ less than $d$ \emph{in parallel} add $f(v, i)$ to the list corresponding to $v$.
  This algorithm can be computed with $dn$ processors, which is polynomial in $n$.
  Since the $f(v, i)$ can be computed in time polynomial in $\log n$, the running time for each parallel processor is also polynomial in $\log n$.
  Therefore we have presented an $\NC$ algorithm which correctly computes a representation of the graph $G$.
\end{proof}

This proposition allows us to replace any polynomial-time fully explicit expander %% in Dinur's proof
with an $\NC$ mildly explicit one.
Dinur's proof only requires only polynomial-time mildly explicit expanders, but replacing that requirement with fully explicit ones harms neither the correctness nor the efficiency of the construction.
Polynomial-time fully explicit constant degree expander graphs exist; see \autocite{rvw00}, for example.

Now, let us return to the preprocessing function $X$, which is defined in two parts, \autocite[Definition~4.1]{dinur07} and \autocite[Definition~4.2]{dinur07}.
In the first part, each vertex $v$ is replaced by an $\NC$ mildly explicit $d$-regular expander on $\deg(v)$ vertices in which the constraints on the edges of the expander are the equality constraint.
In the second part, a constant number of self-loops along with the edges of a $d'$-regular $\NC$ mildly explicit expander on $n$ vertices are added to the graph with null constraints on the added edges.
Both of these parts are computable by an $\NC$ algorithm; note that the size of the output graph in each case is linear in the size of the input graph, so a linear number of processors will suffice (with an additional multiplicative factor of a polynomial number of processors when constructing the expander graphs).
We conclude the following.

\begin{lemma}
  The preprocessing function $X$ is computable by an $\NC$ algorithm.
\end{lemma}

Constraint graph powering, defined in \autocite[Section~1.2]{dinur07}, is the standard graph powering operation with an additional operation on the alphabet and the set of constraints.
Graph powering can be viewed as a generalization of computing the transitive closure of a graph; it computes not only whether there is a path joining two nodes but also the number of paths joining them.
Graph powering can be performed by computing the appropriate power of the adjacency matrix of the graph.
This can be computed in $\NC$ because matrix multiplication is in $\NC$, and a constant number of matrix multiplications remains in $\NC$.
If the alphabet is of constant size, the power graph also has an alphabet of constant size, and each of the constraints becomes a new constraint of constant size (see \autocite[Section~1.2]{dinur07} for the constraint construction).
Each of the constraints (which is the same as the number of edges) can be written by a distinct processor in parallel constant time.
We conclude the following.

\begin{lemma}
  For each positive integer $k$ and each positive integer $d$, computing the $k$th power of a $d$-regular constraint graph can be performed by an $\NC$ algorithm.
\end{lemma}

The assignment testing composition \autocite[Definition~5.1]{dinur07} consists of two parts.
In the first part, each constraint is transformed into a Boolean circuit of constant size.
In the second part, each circuit constructed in this way is provided as input to a computable assignment tester function (which we know exists \autocite[Theorem~5.1]{dinur07}), and the output graph is the union of the output of all the assignment testers.
Since the size of the input to the assignment tester is constant, the assignment tester need only be computable.
Hence, each constraint can be processed this way, in parallel, in constant time with respect to the size of the input graph.

\begin{lemma}
  The assignment testing composition is computable by an $\NC$ algorithm.
\end{lemma}

Since the preprocessing function, constraint graph powering, and assignment testing composition are all computable by an $\NC$ algorithm, we conclude the following.

\begin{lemma}\label{lem:reduction}
  There is a positive integer $q$ such that \textsc{$\frac{1}{2}$-gap $q$-CSP} is hard for $\NP$ under $\NC$ many-one reductions.
\end{lemma}

\autoref{thm:pcpnp}, restated here, follows immediately from \autoref{lem:inapprox} and the hardness of the \textsc{$\frac{1}{2}$-gap $q$-CSP} problem \autoref{lem:reduction}.
Therefore we have shown that any decision problem with a polynomial-time verification procedure can be transformed into a probabilistically checkable proof system with a highly parallel verifier using a small amount of randomness and constant query complexity.

\pcpnp*
